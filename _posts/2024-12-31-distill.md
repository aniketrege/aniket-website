---
layout: distill
title: Matryoshka Representation Learning (MRL)
description: What do these scary sounding words mean?
tags: ml embeddings flexibility
giscus_comments: true
date: 2024-01-31
featured: true

authors:
  - name: Aniket Rege
    url: "https://aniketrege.github.io/"
    affiliations:
      name: University of Wisconsin-Madison

bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  # - name: Equations
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: What is MRL?
    subsections:
    - name: Representation Learning
    - name: Practical ML Training
    - name: Matryoshka
  # - name: Footnotes
  # - name: Code Blocks
  # - name: Interactive Plots
  # - name: Layouts
  # - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

# What is MRL?
If you keep yourself updated on OpenAI's blog posts, you may have seen the recent release of [new embedding models](https://openai.com/blog/new-embedding-models-and-api-updates), which included support for _shortening embeddings_, where developers could simply "remove some numbers from the end of a sequence" and still maintain a valid representation for text. Why is this cool?
1. Save a lot of memory (storing the embedding)
2. Improved Search latency (smaller embeddings = faster search)
3. **Critical**: What if the biggest embedding isn't the best? 

This property emerged in the new models due to a mysterious and unspecified "technique". After some [very minor subtweeting](https://twitter.com/jainprateek_/status/1751479439052140622), OpenAI kindly [updated their blog post](https://twitter.com/owencm/status/1751409104713826666) to remove this shroud of mystery to reveal this technique: Matryoshka Representation Learning<d-cite key="kusupati2022matryoshka"></d-cite>, which you should fully understand from this single GIF:

<center><img src="https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExdXNscHUzajM3ejVhbjR5dXJwczE4N2Y4a28wc3plNW9ucjRmN25jZyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/nthoYgQ91Up2u7qmcE/giphy.gif" alt="A gif illustrating Matroyshka (Russian) dolls" width="300" height="300" /></center>

#### What? 
You didn't understand a lot of what you just read? That's okay, let's take a step back.

>Note: For ML Researchers or those familiar with this space, feel free to skip directly to the [Matryoshka](#matryoshka-dolls) section.

## Representation Learning
#### tl;dr
Modern Deep Learning allows us to _learn_ good representations for data, instead of having experts handcraft them. The [Matryoshka Section](#matryoshka) will take closer look at what "good" means.

### How do we Represent Data for Computers?
Let's say you want to build something cool using some data that you have - a fairly general problem we all face every day. With computers, we need a way to represent this data - an image, text, data table, audio, smell (let me know when you figure this one out) in a way computers can understand. Computers understand data with numbers, and we typically arrive at these numbers with some function $f$ that maps the data from its original representation (e.g. an [RGB encoding of an image](https://web.stanford.edu/class/cs101/image-1-introduction.html#:~:text=Each%20of%20the%20red%2C%20green,in%20a%20shade%20of%20orange)) to a sequence of $d$ numbers, which we also call the **dimensionality** of the data.

$$
\begin{align*}
&f(x = \text{cat image}) = [\text{num}_1, \text{num}_2, \text{num}_3, ..., \text{num}_d], f(x) \in \mathbb{R}^d\\\\
&\text{For example, } f = \text{HoG}(x), f = \text{Canny}(x), f=\text{Gabor}(x)
\end{align*}
$$

With me so far? Now how do we pick a good function $f$ to represent our cat image? In the "old days" (if you've read ML research before 2012, I'm crying with you), expert humans would have done the Representation Learning for everyone else, i.e. used a PhD-worth of domain-specific knowledge to hand-craft good features to represent a cat. For example, maybe we care about horizontal and vertical edges ([Canny Edge Detector](https://en.wikipedia.org/wiki/Canny_edge_detector)), or some kind of filter to analyze image texture ([Gabor Filter](https://en.wikipedia.org/wiki/Gabor_filter)), or maybe something super fancy sounding like a [Histogram of Oriented Gradients](https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients) (HoG). There's one slight problem, what do we do if we arent a domain expert with years of research experience?

Enter this story's main character: ```Machine Learning```.

What if we could let a machine _learn_ this function $f$ from the data? That way, we wouldn't need image processing PhDs to come up with fancy specialized featurizers that, while interpretable, don't generalize well to new kinds of data. 

{% details Click here to know more about the History of Learning $f$ from Data %}
For Images, this function class $f$ was dominated for a long time by [Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network)(CNNs), after the CNN [AlexNet](https://en.wikipedia.org/wiki/AlexNet#:~:text=AlexNet%20is%20the%20name%20of,D.) kicked off the Deep Learning revolution in 2012. The introduction of the [Transformer architecture](https://huggingface.co/learn/nlp-course/en/chapter1/4)<d-cite key="vaswani2017attention"></d-cite>, which revolutionized machine learning for text data in 2017, made its way to the image domain in 2021 with Google's [Vision Transformer](https://huggingface.co/docs/transformers/en/model_doc/vit)<d-cite key="dosovitskiy2020image"></d-cite> work. These modern Deep Learning methods are also called ```Neural Encoders``` as they learn an encoding of the data that computers can work with using Neural Networks. 

If you'd like to learn more about how popular Neural Encoders learn good functions $f$, I heartily recommend an excellent series of blogs from [Jay Alammar](https://twitter.com/JayAlammar), especially the basics of [Neural Networks](https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/) and the [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).
{% enddetails %}

This is great, now we can offload the expertise of crafting features to a neural network that can learn an encoder function $f$ and a learning algorithm $\mathcal{F}$ that is able to search the space of all possible functions to find (hopefully) a good $f$. But what does it mean to learn a "good" $f$? Let's take a closer look, and motivate the "Matryoshka" portion of MRL.

## Practical ML Training
#### tl;dr
We use proxy "objective functions" to train our models with the hope that they achieve our actual goals.

We train modern Machine Learning models (since 2012, these are typically "Deep", i.e. very large, and always getting larger!) with some human interpretable goal. For example, we may want a Face Recognition Model to correctly identify employees of a business entering the premises with 99% accuracy. How do we train a model to achieve this? In ML land, we use an objective function, or "loss" to steer our initial (bad) model to update itself iteratively and hopefully do slightly better each time until we hit the 99% accuracy we require. 

{% details Click here to know more about Optimization, the Study of how to Train Good ML Models %}
There exists rich literature in optimization, the study of how to train machine learning models well, which typically means with some guarantees on performance. With modern Deep Learning methods, these theoretical guarantees become trickier to achieve, and yet they seem to empirically work well with lots of good quality data and scale. The prevalent optimization methods that work well are ```gradient-based```, which simply stated means you find the most promising "direction" for the model to update itself, and take a small step in that direction with every training iteration. 

This promising direction is the negative gradient, i.e. the [derivative](https://en.wikipedia.org/wiki/Derivative) of the loss with respect to the weights of the model. What this means is that the objective we choose has to be ```differentiable```, or we won't be able to figure out which direction we need to travel to get better predictions. Hopefully the ubiquity of this relatively simple calculus in nearly **all** modern machine learning would make Isaac Newton very happy (or Gottfried Leibniz if you [swing that way](https://en.wikipedia.org/wiki/Leibniz%E2%80%93Newton_calculus_controversy)). 
{% enddetails %}

Let's look at a very simple 2-layer neural network whose goal is to predict if an image is a cat or not. Don't get scared by this math, I explain what these things mean below :cat:

$$
\begin{align*}
x &\to \text{Image of a cat} \\
y_{true} &= \text{Is this image a cat or not?} \in \{1, 0\} = \{\text{(yes)}, \text{(no)} \} \\\\

z &= f(x) = W_2 \cdot ReLU(W_1 \cdot x) \in\mathbb{R}^d \\
y_{guess} &= \text{softmax}(z) \\
\end{align*}
$$

To explain this scary notation: $f$ is our neural encoder from the [Representation Learning](#how-do-we-represent-data-for-computers) section, which we choose to model as the simple 2-layer Neural Network above. We have two layers $W_1$ and $W_2$ with a Rectified Linear Unit (ReLU) as an [activation function](https://en.wikipedia.org/wiki/Activation_function) in the middle (don't worry too much about this, it just gives our network a little more 'capacity' to learn potentially better functions $f$). Running our input image through this network gives us our learned representation $z$, a sequence of $d$ [real numbers](https://en.wikipedia.org/wiki/Real_number) (written in *math language* as $z\in\mathbb{R}^d$). Finally, we attach a [softmax function](https://en.wikipedia.org/wiki/Softmax_function) at the end, which will output two probabilities: $p_\text{cat}$ and $p_\text{not-cat}$, where $p_\text{cat} + p_\text{not-cat} = 1$. We consider whichever probability is higher to be our network's "guess" for whether $x$ was a cat image or not, i.e. $y_\text{guess}$.

Let's say we choose a good loss to train our model, e.g. a simple [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy) (not important to understand this example). If we test our trained model on 50 unseen cats and 50 unseen dogs, we would hope that $y_{guess} = y_{true}$ on at least 99 of them, to hit our 99% accuracy goal. Take a second to think of what a "bad" model would do in our cat classifier case.

{% details What accuracy will a "bad" binary classification model give? %}
The worst model is one that is completely uncertain about its prediction, because it gives us no useful information about our data. For binary classification, i.e. with two classes (cat vs. not cat), complete uncertainty means our model guesses randomly between cat and not cat, i.e. $\text{accuracy} = 50\%$. What is a "bad" classifier if we had ten classes?
{% enddetails %}

Your (suggested) takeaway from this Section should be: 
>Huh, our **human goal** was to classify cats correctly 99% of the time, but our **machine objective** was this scary sounding ```cross-entropy```. What gives? Why can't we train our model with a ```git-gud-at-cats``` loss? 

This is the crux of modern optimization methods: a ```misalignment``` between human goals and the methods we use to train models. In our cat case, we hope that the "cross entropy" is a good proxy for our actual human-specified goal. We will consider this *misaligned setting* for the rest of this article, but I do provide some optional further reading below on very popular recent efforts towards directly optimizing for human preferences.

{% details Click here to read about Modern Efforts to Directly Optimize Human Preferences %}
There has been a lot of recent effort towards **directly** aligning large Machine Learning models to human goals, especially in the realm of Large Language Models, with Reinforcement Learning (RL). This [excellent blog post](https://huggingface.co/blog/rlhf) from Lambert et. al<d-cite key="lambert2022illustrating"></d-cite> walks through Reinforcement Learning from Human Feedback (RLHF), which is currently the most popular alignment technique. For Computer Vision nerds, this [excellent recent work](https://twitter.com/giffmana/status/1626695378362945541) from Pinto et. al<d-cite key="pinto2023tuning"></d-cite> applies RL techniques to optimize models directly for vision tasks, such as object detection and image captioning.
{% enddetails %}

## Matryoshka :dolls:

Alright, so you now hopefully have a basic understanding of 
1. How we use modern Deep Learning methods to learn good representations of data ([Representation Learning](#representation-learning))
2. Why we train Neural Encoders with proxy loss functions: the _faith_ that we will achieve our human-interpretable goals, without directly optimizing for them ([Practical ML Training](#practical-ml-training))

I'll now talk about a slightly different problem in modern Machine Learning: 
>How can we learn the generally "best" representation for some given data, and does one even exist?

{% details Click here to Think more about this Question %}
But first, another question: instead of training our simple [cat classifier example](#practical-ml-training) above, can we just use a large ["foundation" model](https://en.wikipedia.org/wiki/Foundation_model) that someone has already trained that has a good understanding of animals, and somehow *transfer* that knowledge to directly guess whether an image is a cat? (Some examples include Google's [ViT](https://blog.research.google/2023/03/scaling-vision-transformers-to-22.html), Meta's [DINO](https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/), OpenAI's [CLIP](https://openai.com/research/clip) and Microsoft's [LLaVa](https://www.microsoft.com/en-us/research/project/llava-large-language-and-vision-assistant/)) This process is called [transfer learning](https://www.v7labs.com/blog/transfer-learning-guide), and is a huge part of what makes modern Deep Learning accessible to researchers and smaller companies with limited resources: we can't all afford to [spend millions of dollars](https://www.forbes.com/sites/craigsmith/2023/09/08/what-large-models-cost-you--there-is-no-free-ai-lunch/?sh=7f09d5e24af7) training our models!

So how DO we know how "good" a representation is? Do we just have faith in our corporate overlords? Maybe Let's use that as a backup option. Let's instead define a notion of "goodness" which is directly tied to tasks we care about; after all, we want our representations to be practically useful. For example, the [Visual Task Adaptation Benchmark](https://google-research.github.io/task_adaptation/) (VTAB) is a suite of 19 tasks designed to test how generally "good" a visual representation is on things it has not been trained on, which is sometimes called [generalizability](https://developers.google.com/machine-learning/crash-course/generalization/video-lecture) or [robustness](https://en.wikipedia.org/wiki/Robustness_(computer_science)) of representations. This is a great starting point, i.e. exhaustive benchmarking and evaluation! Is this our holy grail, the "best" representation? Spoiler: it's not quite that simple!
{% enddetails %}

### What Led to MRL?
Recall that [we said above](#how-do-we-represent-data-for-computers) that the representation learned by our Neural Encoder for our input data $x$ is a sequence of $d$ numbers, i.e. $z = f(x) \in \mathbb{R}^d$. I now ask you the question that led to Matryoshka Representation Learning:

> What is the best choice of data dimensionality $d$ to learn a "good" representation? And is this the same value for all kinds of data? 
> If your answer to this question was **Hmm probably not** then your thought process is exactly where we (the MRL authors) were in Late 2021.

Let's illustrate this idea concretely with an example from the MRL paper<d-cite key="kusupati2022matryoshka"></d-cite>, Figure 9a. The leftmost image in the row is **GT: Sweatshirt**, which is the *Ground Truth* (GT) of the data, i.e. what we consider the *true* label, $y_\text{true} =$ **Sweatshirt**. You can think of the other 4 images as what the model is "$\textcolor{green}{\text{looking at}}$" to make a decision about what this image represents.  Each of these 4 images is using a different $\textcolor{orange}{d}$-dimensional representation of the image to make its decision, with $d \in (\textcolor{orange}{8, 16, 32, 2048})$, and the predicted label $y_\text{pred}$ above each image. We can think of a larger $\textcolor{orange}{d}$ value as being able to represent "more information" about the image (because there are more numbers to represent this information!)

<center><img src="/assets/img/blog/mrl-embedding-capacity.png" alt="Demonstrating the embedding capacity required by varying complexity of images" width="700" height="200" /></center>

As we can see, with very small dimensionality $\textcolor{orange}{d}$, the model makes a mistake and thinks the image is **Sunglasses**, which we can see with $\textcolor{green}{\text{where the model is looking}}$. When we increase $\textcolor{orange}{d=32}$, the model is able to shift its focus more to **Sweatshirt** and get the prediction correct, and it stays correct until $\textcolor{orange}{d=2048}$. This means we could have easily just used a $\dfrac{2048}{32} = 64\times$ smaller embedding to correctly predict this image! 

It makes sense to use the smallest $d$ that works for every data point, because we can save memory (less numbers = less memory) and run faster inference, i.e. compute $y_\text{guess}$ as shown in the [ML Training](#practical-ml-training) section.

There's one problem: machine learning models are trained with fixed data dimensionality $d$. For ResNet-50<d-cite key="he2016deep"></d-cite>, an extremely popular CNN, $d=2048$. For OpenAI's [latest embedding model](https://openai.com/blog/new-embedding-models-and-api-updates) `text-embedding-3-large`, $d=3072$. If we want a smaller $d$, the prevalent methods were to use traditional [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) techniques, such as [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis). The problem with these methods is that they operate "post-hoc" after our $d$-dimensional embedding as been learned by our Neural Encoder, and are thus not data-aware or learned. Is there a way to automatically learn these lower dimensional embeddings without training separate models every time?

### Finally, Enter MRL
MRL learns these lower-dimensional embeddings baked into the original embedding, just like a series of Matryoshka Dolls! For example, the $\textcolor{red}{\text{smallest doll}}$ represents $d=8$, which sits inside a $\textcolor{orange}{\text{slightly larger doll}}$ with $d=16$, which sits inside an $\textcolor{blue}{\text{even larger doll}}$, until we reach the $\textcolor{Gray}{\text{\textbf{largest doll}}}$ with $d=2048$.

<center><img src="/assets/img/blog/mrl-method.png" alt="Demonstrating the MRL methodology" width="600" height="300" /></center>

<!-- ---

## Footnotes

Just wrap the text you would like to show up in a footnote in a `<d-footnote>` tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote>

---

## Code Blocks

Syntax highlighting is provided within `<d-code>` tags.
An example of inline code snippets: `<d-code language="html">let x = 10;</d-code>`.
For larger blocks of code, add a `block` attribute:

<d-code block language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

**Note:** `<d-code>` blocks do not look good in the dark mode.
You can always use the default code-highlight using the `highlight` liquid tag:

{% highlight javascript %}
var x = 25;
function(x) {
return x \* x;
}
{% endhighlight %}

--- -->

<!-- ## Interactive Plots

You can add interative plots using plotly + iframes :framed_picture:

<div class="l-page">
  <iframe src="{{ '/assets/plotly/demo.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

The plot must be generated separately and saved into an HTML file.
To generate the plot that you see above, you can use the following code snippet:

{% highlight python %}
import pandas as pd
import plotly.express as px
df = pd.read_csv(
'https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv'
)
fig = px.density_mapbox(
df,
lat='Latitude',
lon='Longitude',
z='Magnitude',
radius=10,
center=dict(lat=0, lon=180),
zoom=0,
mapbox_style="stamen-terrain",
)
fig.show()
fig.write_html('assets/plotly/demo.html')
{% endhighlight %}

---

## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

--- -->

<!-- ## Other Typography?

Emphasis, aka italics, with _asterisks_ (`*asterisks*`) or _underscores_ (`_underscores_`).

Strong emphasis, aka bold, with **asterisks** or **underscores**.

Combined emphasis with **asterisks and _underscores_**.

Strikethrough uses two tildes. ~~Scratch this.~~

1. First ordered list item
2. Another item
   ⋅⋅\* Unordered sub-list.
3. Actual numbers don't matter, just that it's a number
   ⋅⋅1. Ordered sub-list
4. And another item.

⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).

⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)

- Unordered list can use asterisks

* Or minuses

- Or pluses

[I'm an inline-style link](https://www.google.com)

[I'm an inline-style link with title](https://www.google.com "Google's Homepage")

[I'm a reference-style link][Arbitrary case-insensitive reference text]

[You can use numbers for reference-style link definitions][1]

Or leave it empty and use the [link text itself].

URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <http://www.example.com> and sometimes
example.com (but not on Github, for example).

Some text to show that the reference links can follow later.

[arbitrary case-insensitive reference text]: https://www.mozilla.org
[1]: http://slashdot.org
[link text itself]: http://www.reddit.com

Here's our logo (hover to see the title text):

Inline-style:
![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 1")

Reference-style:
![alt text][logo]

[logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 2"

Inline `code` has `back-ticks around` it.

```javascript
var s = "JavaScript syntax highlighting";
alert(s);
```

```python
s = "Python syntax highlighting"
print s
```

```
No language indicated, so no syntax highlighting.
But let's throw in a <b>tag</b>.
```

Colons can be used to align columns.

| Tables        |      Are      |  Cool |
| ------------- | :-----------: | ----: |
| col 3 is      | right-aligned | $1600 |
| col 2 is      |   centered    |   $12 |
| zebra stripes |   are neat    |    $1 |

There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don't need to make the
raw Markdown line up prettily. You can also use inline Markdown.

| Markdown | Less      | Pretty     |
| -------- | --------- | ---------- |
| _Still_  | `renders` | **nicely** |
| 1        | 2         | 3          |

> Blockquotes are very handy in email to emulate reply text.
> This line is part of the same quote.

Quote break.

> This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can _put_ **Markdown** into a blockquote.

Here's a line for us to start with.

This line is separated from the one above by two newlines, so it will be a _separate paragraph_.

This line is also a separate paragraph, but...
This line is only separated by a single newline, so it's a separate line in the _same paragraph_. -->
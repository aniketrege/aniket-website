<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Matryoshka Representation Learning (MRL) from the Ground Up | Aniket  Rege</title>
    <meta name="author" content="Aniket  Rege">
    <meta name="description" content="What do these scary sounding words mean?">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://aniketrege.github.io/blog/2024/mrl/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Matryoshka Representation Learning (MRL) from the Ground Up",
      "description": "What do these scary sounding words mean?",
      "published": "January 31, 2024",
      "authors": [
        {
          "author": "Aniket Rege",
          "authorURL": "https://aniketrege.github.io/",
          "affiliations": [
            {
              "name": "University of Wisconsin-Madison",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Aniket </span>Rege</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">bio</a>
              </li>
              
              <!-- Blog -->
              <!-- <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li> -->

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/code/">code</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Matryoshka Representation Learning (MRL) from the Ground Up</h1>
        <p>What do these scary sounding words mean?</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#what-is-mrl">What is MRL?</a></div>
            <ul>
              <li><a href="#representation-learning">Representation Learning</a></li>
              <li><a href="#practical-ml-training">Practical ML Training</a></li>
              
            </ul>
<div><a href="#matryoshka">Matryoshka</a></div>
            <ul>
              <li><a href="#what-led-to-creating-mrl">What Led to Creating MRL?</a></li>
              <li><a href="#what-is-mrl-really-this-time">What is MRL? (Really this Time)</a></li>
              <li><a href="#how-good-is-mrl">How Good is MRL?</a></li>
              
            </ul>
<div><a href="#using-mrl">Using MRL</a></div>
            <ul>
              <li><a href="#web-scale-search">Web-Scale Search</a></li>
              <li><a href="#so-what-is-the-catch">So What is the Catch?</a></li>
              <li><a href="#open-source-mrl-models">Open-Source MRL Models</a></li>
              
            </ul>
<div><a href="#thoughts-get-in-touch">Thoughts? Get in Touch!</a></div>
            <div><a href="#further-reading">Further Reading</a></div>
            
          </nav>
        </d-contents>

        <blockquote>
  <p>This article is best suited for ML practitioners and researchers, but is written in an attempt to be accessible to anyone interested in machine learning, artificial intelligence, and tech.
Reading Time Estimates:</p>
  <ol>
    <li>ML Practitioner: 20 minutes</li>
    <li>Working in Computer Science/Tech: 30 minutes</li>
    <li>Generally Interested: 40 minutes+</li>
  </ol>
</blockquote>

<h1 id="what-is-mrl">What is MRL?</h1>
<p>If you keep yourself updated on OpenAI’s blog posts, you may have seen the recent release of <a href="https://openai.com/blog/new-embedding-models-and-api-updates" rel="external nofollow noopener noopener noreferrer" target="_blank">new embedding models</a>, which included support for <em>shortening embeddings</em>, where developers could simply “remove some numbers from the end of a sequence” and still maintain a valid representation for text. Why is this cool?</p>
<ol>
  <li>Save a lot of memory (storing the embedding)</li>
  <li>Improved Search latency (smaller embeddings = faster search)</li>
  <li>
<strong>Critical</strong>: What if the biggest embedding isn’t the best?</li>
</ol>

<p>This property emerged in the new models due to a mysterious and unspecified “technique”. After some <a href="https://twitter.com/jainprateek_/status/1751479439052140622" rel="external nofollow noopener noopener noreferrer" target="_blank">very minor subtweeting</a>, OpenAI kindly <a href="https://twitter.com/owencm/status/1751409104713826666" rel="external nofollow noopener noopener noreferrer" target="_blank">updated their blog post</a> to remove this shroud of mystery to reveal this technique: <img class="emoji" title=":nesting_dolls:" alt=":nesting_dolls:" src="https://github.githubassets.com/images/icons/emoji/unicode/1fa86.png" height="20" width="20"> Matryoshka Representation Learning<d-cite key="kusupati2022matryoshka"></d-cite> <img class="emoji" title=":nesting_dolls:" alt=":nesting_dolls:" src="https://github.githubassets.com/images/icons/emoji/unicode/1fa86.png" height="20" width="20">, which you should fully understand from this single GIF:</p>

<center><img src="https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExdXNscHUzajM3ejVhbjR5dXJwczE4N2Y4a28wc3plNW9ucjRmN25jZyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/nthoYgQ91Up2u7qmcE/giphy.gif" alt="A gif illustrating Matroyshka (Russian) dolls" width="300" height="300"></center>

<h4 id="what">What?</h4>
<p>You didn’t understand a lot of what you just read? That’s okay, let’s take a step back.</p>

<blockquote>
  <p>Note: For ML Researchers or those familiar with this space, feel free to skip directly to the <a href="#matryoshka">Matryoshka</a> <img class="emoji" title=":nesting_dolls:" alt=":nesting_dolls:" src="https://github.githubassets.com/images/icons/emoji/unicode/1fa86.png" height="20" width="20"> section.</p>
</blockquote>

<h2 id="representation-learning">Representation Learning</h2>
<h4 id="tldr">tl;dr</h4>
<p>Modern Deep Learning allows us to <em>learn</em> good representations for data, instead of having experts handcraft them. The <a href="#matryoshka">Matryoshka Section</a> will take closer look at what “good” means.</p>

<h3 id="how-do-we-represent-data-for-computers">How do we Represent Data for Computers?</h3>
<p>Let’s say you want to build something cool using some data that you have - a fairly general problem we all face every day. With computers, we need a way to represent this data - an image, text, data table, audio - in a way computers can understand. Computers understand data with numbers, and we typically arrive at these numbers with some function $\textcolor{gray}{f}$ that maps the data from its original representation (e.g. an <a href="https://web.stanford.edu/class/cs101/image-1-introduction.html#:~:text=Each%20of%20the%20red%2C%20green,in%20a%20shade%20of%20orange" rel="external nofollow noopener noopener noreferrer" target="_blank">RGB encoding of an image</a>) to a sequence of $\textcolor{gray}{d}$ numbers, which we call the <code class="language-plaintext highlighter-rouge">learned representation</code> $\textcolor{gray}{z}$. In <em>math language</em>  we say $\textcolor{gray}{z\in\mathbb{R}^d}$, or representation $\textcolor{gray}{z}$ belongs to the set of <a href="https://en.wikipedia.org/wiki/Real_number" rel="external nofollow noopener noopener noreferrer" target="_blank">real numbers</a> $\textcolor{gray}{\mathbb{R}}$, with <code class="language-plaintext highlighter-rouge">dimensionality</code> $\textcolor{gray}{d}$.</p>

\[\begin{align*}
&amp;x = \text{cat image} \\
&amp;z = f(x) = [\text{num}_1, \text{num}_2, \text{num}_3, ..., \text{num}_d]\\\\
&amp;\text{Examples:} \\
&amp;z = [0.42, -1.96, ..., 1.43],~z \in \mathbb{R}^d \\
&amp;f\to \text{can be HoG, Canny, Gabor}
\end{align*}\]

<p>I know that was a lot of notation, so I hope you’re still with me! Take a minute to understand this process, with the help of the <a href="#what-is-machine-learning">diagram below</a> for a visual overview.</p>

<p>Now how do we pick a good function $\textcolor{gray}{f}$ to represent our cat image? In the “old days” (if you’ve read ML research before 2012, I’m crying with you), expert humans would have done the Representation Learning for everyone else, i.e. used a PhD-worth of domain-specific knowledge to hand-craft good features to represent a cat. For example, maybe we care about</p>
<ol>
  <li>Horizontal and vertical edges (<a href="https://en.wikipedia.org/wiki/Canny_edge_detector" rel="external nofollow noopener noopener noreferrer" target="_blank">Canny Edge Detector</a>)</li>
  <li>Image texture (<a href="https://en.wikipedia.org/wiki/Gabor_filter" rel="external nofollow noopener noopener noreferrer" target="_blank">Gabor Filter</a>)</li>
  <li>Something super fancy sounding (<a href="https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients" rel="external nofollow noopener noopener noreferrer" target="_blank">Histogram of Oriented Gradients</a> - HoG)</li>
</ol>

<p>There’s one slight problem, what do we do if we arent a domain expert with years of research experience? Enter this story’s main character: <code class="language-plaintext highlighter-rouge">Machine Learning</code>.</p>

<h3 id="what-is-machine-learning">What is Machine Learning?</h3>

<p>What if we could let a machine <em>learn</em> this function $\textcolor{gray}{f}$ from the data? That way, we wouldn’t need image processing PhDs to come up with fancy specialized featurizers (like Canny, Gabor, and HoG) that, while we understand what they are doing, they don’t generally work well for new kinds of data.</p>

<details><summary>Click here to know more about the History of Learning $\textcolor{gray}{f}$ from Data</summary>
<p>For Images, this function class $\textcolor{gray}{f}$ was dominated for a long time by <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="external nofollow noopener noopener noreferrer" target="_blank">Convolutional Neural Networks</a>(CNNs), after the CNN <a href="https://en.wikipedia.org/wiki/AlexNet#:~:text=AlexNet%20is%20the%20name%20of,D." rel="external nofollow noopener noopener noreferrer" target="_blank">AlexNet</a> kicked off the Deep Learning revolution in 2012. The introduction of the <a href="https://huggingface.co/learn/nlp-course/en/chapter1/4" rel="external nofollow noopener noopener noreferrer" target="_blank">Transformer architecture</a><d-cite key="vaswani2017attention"></d-cite>, which revolutionized machine learning for text data in 2017, made its way to the image domain in 2021 with Google’s <a href="https://huggingface.co/docs/transformers/en/model_doc/vit" rel="external nofollow noopener noopener noreferrer" target="_blank">Vision Transformer</a><d-cite key="dosovitskiy2020image"></d-cite> work. These modern Deep Learning methods are also called <code class="language-plaintext highlighter-rouge">Neural Encoders</code> as they learn an encoding of the data that computers can work with using Neural Networks.</p>

<p>If you’d like to learn more about how popular Neural Encoders learn good functions $\textcolor{gray}{f}$, I heartily recommend an excellent series of blogs from <a href="https://twitter.com/JayAlammar" rel="external nofollow noopener noopener noreferrer" target="_blank">Jay Alammar</a>, especially the basics of <a href="https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/" rel="external nofollow noopener noopener noreferrer" target="_blank">Neural Networks</a> and the <a href="https://jalammar.github.io/illustrated-transformer/" rel="external nofollow noopener noopener noreferrer" target="_blank">Illustrated Transformer</a>.</p>
</details>

<center><img src="/assets/img/blog/searching_for_functions.png" alt="A simple mechanism to use a learning algorithm to search the function space for a good representation of data" width="600" height="300"></center>
<p><br><br></p>

<p>This is great, now we can offload the expertise of crafting features to a neural network that can learn an encoder function $\textcolor{gray}{f}$. As seen in the figure above, this workflow involves a learning algorithm $\textcolor{gray}{\mathcal{A}}$ that is able to search the space of all possible functions $\textcolor{gray}{\mathcal{F}}$ to learn (hopefully) a good representation of the data $\textcolor{gray}{z=f(x)}$, i.e. a sequence of $\textcolor{gray}{d}$ numbers our computer can understand.</p>

<p>But what does it mean to learn a “good” representation $\textcolor{gray}{z}$? This question was the inspiration for <code class="language-plaintext highlighter-rouge">Matryoshka</code>.</p>

<h2 id="practical-ml-training">Practical ML Training</h2>
<h4 id="tldr-1">tl;dr</h4>
<p>We use proxy “objective functions” to train our models with the hope that they achieve our actual goals.</p>

<h3 id="how-do-we-train-machine-learning-models">How do we Train Machine Learning Models?</h3>
<p>We train modern Machine Learning models (since 2012, these are typically “Deep”, i.e. very large, and always getting larger!) with some human interpretable goal. For example, we may want a Face Recognition Model to correctly identify employees of a business entering the premises with 99% accuracy. How do we train a model to achieve this? In ML land, we use an objective function, or “loss” to steer our initial (bad) model to update itself iteratively and hopefully do slightly better each time until we hit the 99% accuracy we require.</p>

<details><summary>Click here to know more about Optimization, the Study of how to Train Good ML Models</summary>
<p>There exists rich literature in optimization, the study of how to train machine learning models well, which typically means with some guarantees on performance. With modern Deep Learning methods, these theoretical guarantees become trickier to achieve, and yet they seem to empirically work well with lots of good quality data and scale. The prevalent optimization methods that work well are <code class="language-plaintext highlighter-rouge">gradient-based</code>, which simply stated means you find the most promising “direction” for the model to update itself, and take a small step in that direction with every training iteration.</p>

<p>This promising direction is the negative gradient, i.e. the <a href="https://en.wikipedia.org/wiki/Derivative" rel="external nofollow noopener noopener noreferrer" target="_blank">derivative</a> of the loss with respect to the weights of the model. What this means is that the objective we choose has to be <code class="language-plaintext highlighter-rouge">differentiable</code>, or we won’t be able to figure out which direction we need to travel to get better predictions. Hopefully the ubiquity of this relatively simple calculus in nearly <strong>all</strong> modern machine learning would make Isaac Newton very happy (or Gottfried Leibniz if you <a href="https://en.wikipedia.org/wiki/Leibniz%E2%80%93Newton_calculus_controversy" rel="external nofollow noopener noopener noreferrer" target="_blank">swing that way</a>).</p>
</details>

<p>Let’s look at a very simple 2-layer neural network whose goal is to predict if an image is a cat or not. Don’t get scared by this math, I explain what these things mean below <img class="emoji" title=":cat:" alt=":cat:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f431.png" height="20" width="20"></p>

\[\begin{align*}
x &amp;\to \text{Image of a cat} \\
y_{true} &amp;= \text{Is this image a cat or not?} \in \{1, 0\} = \{\text{(yes)}, \text{(no)} \} \\\\

z &amp;= f(x) = W_2 \cdot ReLU(W_1 \cdot x) \in\mathbb{R}^d \\
y_{guess} &amp;= \text{softmax}(z) \\
\end{align*}\]

<p>To explain this scary notation: $\textcolor{gray}{f}$ is our neural encoder from the <a href="#how-do-we-represent-data-for-computers">Representation Learning</a> section, which we choose to model as the simple 2-layer Neural Network above. We have two layers $\textcolor{gray}{W_1}$ and $\textcolor{gray}{W_2}$ with a Rectified Linear Unit (ReLU) as an <a href="https://en.wikipedia.org/wiki/Activation_function" rel="external nofollow noopener noopener noreferrer" target="_blank">activation function</a> in the middle (don’t worry too much about this, it just gives our network a little more ‘capacity’ to learn potentially better functions $\textcolor{gray}{f}$). Running our input image through this network gives us our learned representation $\textcolor{gray}{z}$, a sequence of $\textcolor{gray}{d}$ <a href="https://en.wikipedia.org/wiki/Real_number" rel="external nofollow noopener noopener noreferrer" target="_blank">real numbers</a> (written in <em>math language</em> as $\textcolor{gray}{z\in\mathbb{R}^d}$). Finally, we attach a <a href="https://en.wikipedia.org/wiki/Softmax_function" rel="external nofollow noopener noopener noreferrer" target="_blank">softmax function</a> at the end, which will output two probabilities: $\textcolor{gray}{p_\text{cat}}$ and $\textcolor{gray}{p_\text{not-cat}}$, where $\textcolor{gray}{p_\text{cat} + p_\text{not-cat} = 1}$. We consider whichever probability is higher to be our network’s “guess” for whether $\textcolor{gray}{x}$ was a cat image or not, i.e. $\textcolor{gray}{y_\text{guess}}$. This process is illustrated visually in the diagram below:</p>

<center><img src="/assets/img/blog/simple_nn.png" alt="A simple 2-layer neural network computing a guess for whether an image is a cat or not" width="700" height="300"></center>
<p><br><br></p>

<p>Let’s say we choose a good loss to train our model, e.g. a simple <a href="https://en.wikipedia.org/wiki/Cross-entropy" rel="external nofollow noopener noopener noreferrer" target="_blank">cross-entropy</a> (not important to understand this example). If we test our trained model on 50 unseen cats and 50 unseen dogs, we would hope that $\textcolor{gray}{y_{guess} = y_{true}}$ on at least 99 of them, to hit our 99% accuracy goal. Take a second to think of what a “bad” model would do in our cat classifier case.</p>

<details><summary>What accuracy will a “bad” binary classification model give?</summary>
<p>The worst model is one that is completely uncertain about its prediction, because it gives us no useful information about our data. For binary classification, i.e. with two classes (cat vs. not cat), complete uncertainty means our model guesses randomly between cat and not cat, i.e. $\textcolor{gray}{\text{accuracy} = 50\%}$. What is a “bad” classifier if we had ten classes?</p>
</details>

<p>Your (suggested) takeaway from this Section should be:</p>
<blockquote>
  <p>Huh, our <strong>human goal</strong> was to classify cats correctly 99% of the time, but our <strong>machine objective</strong> was this scary sounding <code class="language-plaintext highlighter-rouge">cross-entropy</code>. What gives? Why can’t we train our model with a <code class="language-plaintext highlighter-rouge">git-gud-at-cats</code> loss?</p>
</blockquote>

<p>This is the crux of modern optimization methods: a <code class="language-plaintext highlighter-rouge">misalignment</code> between human goals and the methods we use to train models. In our cat case, we hope that the “cross entropy” is a good proxy for our actual human-specified goal. We will consider this <em>misaligned setting</em> for the rest of this article, but I do provide some optional further reading below on very popular recent efforts towards directly <code class="language-plaintext highlighter-rouge">optimizing for human preferences</code>, which is the focus of <code class="language-plaintext highlighter-rouge">my current research</code>.</p>

<details><summary>Click here to read about Modern Efforts to Directly Optimize Human Preferences</summary>
<p>There has been a lot of recent effort towards <strong>directly</strong> aligning large Machine Learning models to human goals, especially in the realm of Large Language Models, with Reinforcement Learning (RL). This <a href="https://huggingface.co/blog/rlhf" rel="external nofollow noopener noopener noreferrer" target="_blank">excellent blog post</a> from Lambert et. al<d-cite key="lambert2022illustrating"></d-cite> walks through Reinforcement Learning from Human Feedback (RLHF), which is currently the most popular alignment technique. For Computer Vision nerds, this <a href="https://twitter.com/giffmana/status/1626695378362945541" rel="external nofollow noopener noopener noreferrer" target="_blank">excellent recent work</a> from Pinto et. al<d-cite key="pinto2023tuning"></d-cite> applies RL techniques to optimize models directly for vision tasks, such as object detection and image captioning.</p>
</details>

<h1 id="matryoshka">Matryoshka</h1>

<p>Alright, so you now hopefully have a basic understanding of</p>
<ol>
  <li>How we use modern Deep Learning methods to learn good representations of data (<a href="#representation-learning">Representation Learning</a>)</li>
  <li>Why we train Neural Encoders with proxy loss functions: the <em>faith</em> that we will achieve our human-interpretable goals, without directly optimizing for them (<a href="#practical-ml-training">Practical ML Training</a>)</li>
</ol>

<p>I’ll now talk about a slightly different problem in modern Machine Learning:</p>
<blockquote>
  <p>How can we learn the generally “best” representation for some given data, and does one even exist?</p>
</blockquote>

<details><summary>Click here to Think more about this Question</summary>
<p>But first, another question: instead of training our simple <a href="#practical-ml-training">cat classifier example</a> above, can we just use a large <a href="https://en.wikipedia.org/wiki/Foundation_model" rel="external nofollow noopener noopener noreferrer" target="_blank">“foundation” model</a> that someone has already trained that has a good understanding of animals, and somehow <em>transfer</em> that knowledge to directly guess whether an image is a cat? (Some examples include Google’s <a href="https://blog.research.google/2023/03/scaling-vision-transformers-to-22.html" rel="external nofollow noopener noopener noreferrer" target="_blank">ViT</a>, Meta’s <a href="https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/" rel="external nofollow noopener noopener noreferrer" target="_blank">DINO</a>, OpenAI’s <a href="https://openai.com/research/clip" rel="external nofollow noopener noopener noreferrer" target="_blank">CLIP</a> and Microsoft’s <a href="https://www.microsoft.com/en-us/research/project/llava-large-language-and-vision-assistant/" rel="external nofollow noopener noopener noreferrer" target="_blank">LLaVa</a>) This process is called <a href="https://www.v7labs.com/blog/transfer-learning-guide" rel="external nofollow noopener noopener noreferrer" target="_blank">transfer learning</a>, and is a huge part of what makes modern Deep Learning accessible to researchers and smaller companies with limited resources: we can’t all afford to <a href="https://www.forbes.com/sites/craigsmith/2023/09/08/what-large-models-cost-you--there-is-no-free-ai-lunch/?sh=7f09d5e24af7" rel="external nofollow noopener noopener noreferrer" target="_blank">spend millions of dollars</a> training our models!</p>

<p>So how DO we know how “good” a representation is? Do we just have faith in our corporate overlords? Maybe Let’s use that as a backup option. Let’s instead define a notion of “goodness” which is directly tied to tasks we care about; after all, we want our representations to be practically useful. For example, the <a href="https://google-research.github.io/task_adaptation/" rel="external nofollow noopener noopener noreferrer" target="_blank">Visual Task Adaptation Benchmark</a> (VTAB) is a suite of 19 tasks designed to test how generally “good” a visual representation is on things it has not been trained on, which is sometimes called <a href="https://developers.google.com/machine-learning/crash-course/generalization/video-lecture" rel="external nofollow noopener noopener noreferrer" target="_blank">generalizability</a> or <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)" rel="external nofollow noopener noopener noreferrer" target="_blank">robustness</a> of representations. This is a great starting point, i.e. exhaustive benchmarking and evaluation! Is this our holy grail, the “best” representation? Spoiler: it’s not quite that simple!</p>
</details>

<h2 id="what-led-to-creating-mrl">What Led to Creating MRL?</h2>
<p>Recall that <a href="#how-do-we-represent-data-for-computers">we said above</a> that the representation $\textcolor{gray}{z}$ learned by our Neural Encoder $\textcolor{gray}{f}$ for our input data $\textcolor{gray}{x}$ is a sequence of $\textcolor{gray}{d}$ numbers, i.e. $\textcolor{gray}{z = f(x) \in \mathbb{R}^d}$. I now ask you the question that led to Matryoshka Representation Learning:</p>

<blockquote>
  <p>What is the best choice of data dimensionality $\textcolor{gray}{d}$ to learn a “good” representation? And is this the same value for all kinds of data? 
If your answer to this question was <code class="language-plaintext highlighter-rouge">Hmm probably not</code> then your thought process is exactly where we (the MRL authors) were in Late 2021.</p>
</blockquote>

<p>Let’s illustrate this idea concretely with an example from the MRL paper<d-cite key="kusupati2022matryoshka"></d-cite>, Figure 9a. The leftmost image in the row is <strong>GT: Sweatshirt</strong>, which is the <em>Ground Truth</em> (GT) of the data, i.e. what we consider the <em>true</em> label, $\textcolor{gray}{y_\text{true} =}$ <strong>Sweatshirt</strong>. You can think of the other 4 images as what the model is “$\textcolor{green}{\text{looking at}}$” to make a decision about what this image represents.  Each of these 4 images is using a different $\textcolor{orange}{d}$-dimensional representation of the image to make its decision, with $\textcolor{gray}{d \in} (\textcolor{orange}{8, 16, 32, 2048})$, and the predicted label $\textcolor{gray}{y_\text{pred}}$ above each image. We can think of a larger $\textcolor{orange}{d}$ value as being able to represent “more information” about the image (because there are more numbers to represent this information!)</p>

<center><img src="/assets/img/blog/mrl-embedding-capacity.png" alt="Demonstrating the embedding capacity required by varying complexity of images" width="700" height="200"></center>
<p><br><br></p>

<p>As we can see, with very small dimensionality $\textcolor{orange}{d}$, the model makes a mistake and thinks the image is <strong>Sunglasses</strong>, which we can see with $\textcolor{green}{\text{where the model is looking}}$. When we increase $\textcolor{orange}{d=32}$, the model is able to shift its focus more to <strong>Sweatshirt</strong> and get the prediction correct, and it stays correct until $\textcolor{orange}{d=2048}$. This means we could have easily just used a $\textcolor{gray}{\dfrac{2048}{32} = 64\times}$ smaller embedding to correctly predict this image!</p>

<p>It makes sense to use the smallest $\textcolor{gray}{d}$ that works for every data point, because we can save memory (less numbers = less memory) and run faster inference, i.e. compute $\textcolor{gray}{y_\text{guess}}$ as shown in the <a href="#practical-ml-training">ML Training</a> section.</p>

<p>There’s one problem: machine learning models are trained with fixed data dimensionality $\textcolor{gray}{d}$. For ResNet-50<d-cite key="he2016deep"></d-cite>, an extremely popular CNN, $\textcolor{gray}{d=2048}$. For OpenAI’s <a href="https://openai.com/blog/new-embedding-models-and-api-updates" rel="external nofollow noopener noopener noreferrer" target="_blank">latest embedding model</a> <code class="language-plaintext highlighter-rouge">text-embedding-3-large</code>, $\textcolor{gray}{d=3072}$. If we want a smaller $\textcolor{gray}{d}$, the prevalent methods were to use traditional <a href="https://en.wikipedia.org/wiki/Dimensionality_reduction" rel="external nofollow noopener noopener noreferrer" target="_blank">dimensionality reduction</a> techniques, such as <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="external nofollow noopener noopener noreferrer" target="_blank">Principal Component Analysis</a>. The problem with these methods is that they operate “post-hoc” after our $\textcolor{gray}{d}$-dimensional embedding as been learned by our Neural Encoder, and are thus not data-aware or learned. Is there a way to automatically learn these lower dimensional embeddings without training separate models every time?</p>

<h2 id="what-is-mrl-really-this-time">What is MRL? (Really this Time)</h2>
<p>MRL learns these lower-dimensional embeddings baked into the original embedding, just like a series of Matryoshka Dolls! For example, the $\textcolor{red}{\text{smallest doll}~z_{1:8}}$ represents $\textcolor{red}{d=8}$, which sits inside a $\textcolor{orange}{\text{slightly larger doll}~z_{1:16}}$ with $\textcolor{orange}{d=16}$, which sits inside an $\textcolor{blue}{\text{even larger doll}~z_{1:32}}$, until we reach the $\textcolor{Gray}{\text{\textbf{largest doll}}~z_{1:2048}}$ with $\textcolor{gray}{d=2048}$, as seen in the figure below. Hereon, I will interchangeably use <code class="language-plaintext highlighter-rouge">dolls</code> to refer to <code class="language-plaintext highlighter-rouge">representations learned by MRL</code>.</p>

<center><img src="/assets/img/blog/mrl-method.png" alt="Demonstrating the MRL methodology" width="600" height="300"></center>
<p><br><br></p>

<details><summary>What are those symbols under “Training”?</summary>
<p>MRL is primarily a training paradigm to learn a nested structure of representations, resembling Matryoshka dolls. So how do we train a model to enforce this structure? It’s actually surprisingly simple! We apply the same <code class="language-plaintext highlighter-rouge">cross-entropy loss</code> we would have used for a plain old regular model (just the $\textcolor{Gray}{\text{\textbf{largest doll}}~z_{1:2048}}$) to each doll independently, and average all these losses together:</p>

\[\begin{align*}
\mathcal{L_\text{Regular}} &amp;= \mathcal{L}(z_{1:2048}) \\
\mathcal{L_\text{Matryoshka}} &amp;= \text{average}\left(\mathcal{L}(z_{1:8}) + \mathcal{L}(z_{1:16}) + ... + \mathcal{L}(z_{1:2048})\right)
\end{align*}\]

<p>This simple modification forces the model to learn dolls that are valid and useful representations of the data by themselves! This means we can freely use whichever doll fits our purpose (budget vs accuracy).</p>
</details>

<h2 id="how-good-is-mrl">How Good is MRL?</h2>
<p>You may be wondering, how does learning Matryoshka dolls compare to training a new doll from scratch at different dimensionality $\textcolor{gray}{d}$ every time? While training all dolls at once with MRL is much more efficient, surely each MRL doll’s performance will be <strong>worse</strong> than than its corresponding independently trained doll?</p>

<p>We were pleasantly surprised to discover that $\textcolor{blue}{\text{MRL dolls}}$ outperform $\textcolor{green}{\text{independently trained dolls}}$ at each dimensionality, as seen in the figures below from the MRL paper<d-cite key="kusupati2022matryoshka"></d-cite>, at both:</p>

<p>(a) <code class="language-plaintext highlighter-rouge">Million scale</code> on <a href="https://en.wikipedia.org/wiki/ImageNet" rel="external nofollow noopener noopener noreferrer" target="_blank">ImageNet-1K</a> with a ResNet-50<d-cite key="he2016deep"></d-cite> Neural Encoder at $\textcolor{gray}{d_\text{ImageNet} \in (8, 16, … , 2048)}$</p>

<p>(b) <code class="language-plaintext highlighter-rouge">Billion scale</code> on JFT-300M<d-cite key="sun2017revisiting"></d-cite> with ViT B/16<d-cite key="dosovitskiy2020image"></d-cite> and <a href="https://blog.research.google/2021/05/align-scaling-up-visual-and-vision.html?m=0" rel="external nofollow noopener noopener noreferrer" target="_blank">ALIGN</a> Neural Encoders with $\textcolor{gray}{d_\text{JFT} \in (12, 24, … , 768)}$</p>

<center>
<img src="/assets/img/blog/mrl-r50-imagenet.png" alt="MRL with ResNet50 models on ImageNet show strong performance at all doll sizes" width="400" height="300">
<p>a) ResNet-50 1-NN Accuracy on ImageNet</p>
<img src="/assets/img/blog/mrl-vit-align-jft.png" alt="MRL performance seamlessly scales to billion scale data" width="400" height="300">
<p>b) ViT B/16 and ALIGN 1-NN Accuracy on JFT</p>
</center>

<p>In summary, MRL provides little to no accuracy drop for large efficiency gains across:</p>
<ol>
  <li>
<code class="language-plaintext highlighter-rouge">Data Scale</code> - million to billion</li>
  <li>
<code class="language-plaintext highlighter-rouge">Data Modality</code> - vision, language, vision + language</li>
  <li>
<code class="language-plaintext highlighter-rouge">Neural Encoder Architecture</code> - ResNet-50, ConvNeXt<d-cite key="liu2022convnet"></d-cite>, ViT, <a href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="external nofollow noopener noopener noreferrer" target="_blank">BERT</a>, ALIGN</li>
</ol>

<h1 id="using-mrl">Using MRL</h1>

<h2 id="web-scale-search">Web-Scale Search!</h2>
<p>Now what can we do with this collection of dolls? You might have gotten a hint from the diagram <a href="#finally-enter-mrl">above</a>, but if not, <code class="language-plaintext highlighter-rouge">Matryoshka</code> enables a strong information retrieval setup, just like how Google shows you your favorite cat pics when you search for them! Here’s a walkthrough of what that might look like:</p>

<h3 id="encode-your-data">Encode your Data</h3>
<p>Use the largest doll that fits in your budget to encode all cat pictures on the internet into a database $\textcolor{gray}{X_{Mat}}$. For example, say you have 50 Million cat images (please share them with me) and 100 Gigabytes of storage. With <a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format" rel="external nofollow noopener noopener noreferrer" target="_blank">fp32 precision</a></p>
<ol>
  <li>The smallest doll $\textcolor{gray}{d=8}$ would occupy $\textcolor{gray}{8 * 4 = 32}$ bytes of memory, and thus $\textcolor{gray}{X_{Mat}}$ would occupy $\textcolor{gray}{50,000,000 * 32 = 400}$ Megabytes on disk. We are only using 0.4% of our 100GB budget, we can go much higher!</li>
  <li>The largest doll $\textcolor{gray}{d=2048}$ would occupy $\textcolor{gray}{2048 * 4 = 8192}$ bytes of memory, and thus $\textcolor{gray}{X_{Mat}}$ would occupy $\textcolor{gray}{50,000,000 * 8192 = 409.6}$ Gigabytes on disk. The largest doll is too big to fit into our 100 GB memory budget, we need something in the middle!</li>
  <li>With some <a href="https://www.youtube.com/watch?v=3M_5oYU-IsU&amp;ab_channel=MichaelDapaah" rel="external nofollow noopener noopener noreferrer" target="_blank">quick mafs</a> calculations, we see that we can use a doll of size $\textcolor{gray}{d=500}$, since $\textcolor{gray}{X_{Mat}}$ would occupy $\textcolor{gray}{50,000,000 * 500 * 4 = 100}$ Gigabytes exactly.</li>
</ol>

<h3 id="build-a-search-index">Build a Search Index</h3>
<p>Now that we have encoded our data with MRL, we need a way to search for relevant cat pictures, using a <code class="language-plaintext highlighter-rouge">search index</code> built on top of our database $\textcolor{gray}{X_{Mat}}$! To speak more corporately, Meta uses hierarchical graph indices<d-cite key="malkov2018efficient"></d-cite>, Microsoft uses hybrid SSD-Disk graph indices<d-cite key="jayaram2019diskann"></d-cite>, Google uses tree indices<d-cite key="sivic2003video"></d-cite> and sophisticated Vector Quantization techniques<d-cite key="guo2020accelerating"></d-cite>, to name a few.</p>

<h3 id="searching-a-query">Searching a Query</h3>
<p>Let’s say we want to find the best “Derpy Orange Cat” in our database. We’ll use the search index to find the 50 closest matches (in ML terminology, “Nearest Neighbors”), and hopefully serve a very happy customer! MRL has enabled us to <code class="language-plaintext highlighter-rouge">use a high-dimensional embedding "for free"</code>, since we didn’t have to train or finetune a separate 500-dimensional doll to encode our database, or use dimensionality reduction methods on our largest doll.</p>

<h3 id="hold-on-a-minute-can-we-just-use-any-doll">Hold on a Minute, Can we Just Use any Doll?</h3>
<blockquote>
  <p>You may have already asked the question: Hey, we only trained MRL at specific doll sizes $\textcolor{gray}{d_{ImageNet}}$ and $\textcolor{gray}{d_{JFT}}$. Can we just use any doll of size $\textcolor{gray}{d}$ that doesn’t lie in these specific values we used to train the MRL model?</p>
</blockquote>

<center>
<img src="/assets/img/blog/mrl-interpolate.png" alt="Demonstrating MRL's interpolation behavior at dimensionalities it was not trained on" width="400" height="300">
<p> MRL model accuracies interpolate! </p>
</center>

<p><br><br></p>

<p>It turns that yes, you can - <code class="language-plaintext highlighter-rouge">MRL model accuracies seamlessly interpolate at all doll sizes</code> between the fixed doll sizes it was trained for ($\textcolor{gray}{d_{ImageNet}}$ and $\textcolor{gray}{d_{JFT}}$)! You can see this in the figure from the MRL paper above, where the X-Axis is the doll size or representation size, and all the $\textcolor{red}{\text{red points}}$ are evaluations at $\textcolor{red}{\text{interpolated sizes}}$. This means we can freely, in the <a href="https://openai.com/blog/new-embedding-models-and-api-updates" rel="external nofollow noopener noopener noreferrer" target="_blank">words of OpenAI</a>, “remove some numbers from the end of the sequence” of any representation, and use that embedding directly! I’m not going to make a doll analogy for this because the thought is quite gruesome.</p>

<h2 id="so-what-is-the-catch">So What is the Catch?</h2>
<p>None! Please go train MRL models on huge datasets with huge transformers and open source your work!</p>

<p>And that, dear reader, is the biggest catch: MRL models have a <code class="language-plaintext highlighter-rouge">one-time cost of retraining from scratch</code> to match independently trained models. We discovered that this can be alleviated to a large extent by unfreezing some of the last layers of the Neural Encoder and finetuning for several epochs (See Table 26 in the MRL paper<d-cite key="kusupati2022matryoshka"></d-cite>), but this does not recover from-scratch training performance fully, especially for smaller dolls ($\textcolor{gray}{d \leq 32}$).</p>

<h2 id="open-source-mrl-models">Open-Source MRL Models</h2>
<p>Here are some freely available open-source MRL models to play around with!</p>
<ol>
  <li>ResNet-18/34/50/101 MRL and independent baseline models trained on ImageNet-1K: <a href="https://huggingface.co/aniketr/mrl-resnet50" rel="external nofollow noopener noopener noreferrer" target="_blank">huggingface.co/aniketr/mrl-resnet50</a>
</li>
  <li>ConvNeXt-Tiny trained on ImageNet-1K: <a href="https://huggingface.co/aniketr/mrl-convnext-tiny" rel="external nofollow noopener noopener noreferrer" target="_blank">huggingface.co/aniketr/mrl-convnext-tiny</a>
</li>
  <li>BERT-Base models finetuned on Natural Questions: <a href="https://huggingface.co/aniketr/mrl-nq" rel="external nofollow noopener noopener noreferrer" target="_blank">huggingface.co/aniketr/mrl-nq</a>
</li>
</ol>

<h2 id="thoughts-get-in-touch">Thoughts? Get in Touch!</h2>
<p>I know this was quite a long article, so thanks for sticking around, and I hope you got something useful out of it!</p>

<p>If you’re an ML or CS researcher and have thoughts or questions about our work or improving this article, I would love to have chat about <code class="language-plaintext highlighter-rouge">MRL</code>, our followup work, and my current research.</p>

<p>If you’re someone who is generally interested in our work and found this article interesting, difficult, or relevant, I’d love to hear from you too!</p>

<p>Please get in touch with me via <code class="language-plaintext highlighter-rouge">aniketr@cs.wisc.edu</code> or on my twitter <a href="https://twitter.com/wregss" rel="external nofollow noopener noopener noreferrer" target="_blank">@wregss</a>.</p>

<h2 id="further-reading">Further Reading</h2>
<ol>
  <li>
<a href="https://arxiv.org/abs/2106.01487" rel="external nofollow noopener noopener noreferrer" target="_blank">LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes</a> - learning low-dimensional binary codes for classes and instances for data, e.g. class = “Dog” and instance = a specific German Shepherd Image.</li>
  <li>
<a href="https://arxiv.org/abs/2305.19435" rel="external nofollow noopener noopener noreferrer" target="_blank">AdANNS: A Framework for Adaptive Semantic Search</a> - using adaptive representations (via MRL) to flexibly decouple all stages of a large scale search system and provide strong accuracy-compute tradeoff for all deployment budgets. Currently in use at Google Search.</li>
  <li>
<a href="https://arxiv.org/abs/2310.07707" rel="external nofollow noopener noopener noreferrer" target="_blank">MatFormer: Nested Transformer for Elastic Inference</a> - using MRL in the weight space of a Transformer to extract hundreds of smaller transformers after a single training pass.</li>
  <li>
<a href="https://www.pinecone.io/learn/series/faiss/" rel="external nofollow noopener noopener noreferrer" target="_blank">Pinecone blogs on Vector Search Components</a> - a series of excellent blog posts by <a href="https://twitter.com/jamescalam" rel="external nofollow noopener noopener noreferrer" target="_blank">James Briggs</a> on the various components of vector search at scale, including search space pruning and vector quantization techniques.</li>
</ol>

<!-- ---

## Footnotes

Just wrap the text you would like to show up in a footnote in a `<d-footnote>` tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote>

---

## Code Blocks

Syntax highlighting is provided within `<d-code>` tags.
An example of inline code snippets: `<d-code language="html">let x = 10;</d-code>`.
For larger blocks of code, add a `block` attribute:

<d-code block language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

**Note:** `<d-code>` blocks do not look good in the dark mode.
You can always use the default code-highlight using the `highlight` liquid tag:


<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
<span class="k">return</span> <span class="nx">x</span> <span class="err">\</span><span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>


--- -->

<!-- ## Interactive Plots

You can add interative plots using plotly + iframes :framed_picture:

<div class="l-page">
  <iframe src="/assets/plotly/demo.html" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

The plot must be generated separately and saved into an HTML file.
To generate the plot that you see above, you can use the following code snippet:


<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span>
<span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span>
<span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
<span class="n">df</span><span class="p">,</span>
<span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span>
<span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">assets/plotly/demo.html</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>


---

## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

--- -->

<!-- ## Other Typography?

Emphasis, aka italics, with _asterisks_ (`*asterisks*`) or _underscores_ (`_underscores_`).

Strong emphasis, aka bold, with **asterisks** or **underscores**.

Combined emphasis with **asterisks and _underscores_**.

Strikethrough uses two tildes. ~~Scratch this.~~

1. First ordered list item
2. Another item
   ⋅⋅\* Unordered sub-list.
3. Actual numbers don't matter, just that it's a number
   ⋅⋅1. Ordered sub-list
4. And another item.

⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).

⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)

- Unordered list can use asterisks

* Or minuses

- Or pluses

[I'm an inline-style link](https://www.google.com)

[I'm an inline-style link with title](https://www.google.com "Google's Homepage")

[I'm a reference-style link][Arbitrary case-insensitive reference text]

[You can use numbers for reference-style link definitions][1]

Or leave it empty and use the [link text itself].

URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <http://www.example.com> and sometimes
example.com (but not on Github, for example).

Some text to show that the reference links can follow later.

[arbitrary case-insensitive reference text]: https://www.mozilla.org
[1]: http://slashdot.org
[link text itself]: http://www.reddit.com

Here's our logo (hover to see the title text):

Inline-style:
![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 1")

Reference-style:
![alt text][logo]

[logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 2"

Inline `code` has `back-ticks around` it.

```javascript
var s = "JavaScript syntax highlighting";
alert(s);
```

```python
s = "Python syntax highlighting"
print s
```

```
No language indicated, so no syntax highlighting.
But let's throw in a <b>tag</b>.
```

Colons can be used to align columns.

| Tables        |      Are      |  Cool |
| ------------- | :-----------: | ----: |
| col 3 is      | right-aligned | $1600 |
| col 2 is      |   centered    |   $12 |
| zebra stripes |   are neat    |    $1 |

There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don't need to make the
raw Markdown line up prettily. You can also use inline Markdown.

| Markdown | Less      | Pretty     |
| -------- | --------- | ---------- |
| _Still_  | `renders` | **nicely** |
| 1        | 2         | 3          |

> Blockquotes are very handy in email to emulate reply text.
> This line is part of the same quote.

Quote break.

> This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can _put_ **Markdown** into a blockquote.

Here's a line for us to start with.

This line is separated from the one above by two newlines, so it will be a _separate paragraph_.

This line is also a separate paragraph, but...
This line is only separated by a single newline, so it's a separate line in the _same paragraph_. -->

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Aniket  Rege. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener noopener noreferrer" target="_blank">al-folio</a> theme.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

  
</body>
</html>
